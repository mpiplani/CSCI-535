{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSCI 535 HW 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0myFbDTf_aoV",
        "outputId": "986c8949-30ac-44bf-9c01-189899c3fc7b"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('//content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at //content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8ZIA7Lk_kRC",
        "outputId": "4fde9d70-48a1-4aa0-8ce0-7cd76eebb9b0"
      },
      "source": [
        "cd '/content/gdrive/My Drive/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9Hz-Fn_EFqc"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4Il5F8kEHv8"
      },
      "source": [
        "data=pd.read_csv('dataset.csv')\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XISHX6AnZo3"
      },
      "source": [
        "Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "t2ABm8wzELWY",
        "outputId": "db340efb-aa77-4115-a63c-92200ceaaed3"
      },
      "source": [
        "data.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name_list</th>\n",
              "      <th>speakers</th>\n",
              "      <th>visual_features</th>\n",
              "      <th>acoustic_features</th>\n",
              "      <th>lexical_features</th>\n",
              "      <th>emotion_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1331</th>\n",
              "      <td>Ses05M_script03_2_M029</td>\n",
              "      <td>M05</td>\n",
              "      <td>/features/visual_features/Session5/Ses05M_scri...</td>\n",
              "      <td>/features/acoustic_features/Session5/Ses05M_sc...</td>\n",
              "      <td>/features/lexical_features/Session5/Ses05M_scr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1332</th>\n",
              "      <td>Ses05M_script03_2_M039</td>\n",
              "      <td>M05</td>\n",
              "      <td>/features/visual_features/Session5/Ses05M_scri...</td>\n",
              "      <td>/features/acoustic_features/Session5/Ses05M_sc...</td>\n",
              "      <td>/features/lexical_features/Session5/Ses05M_scr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1333</th>\n",
              "      <td>Ses05M_script03_2_M041</td>\n",
              "      <td>M05</td>\n",
              "      <td>/features/visual_features/Session5/Ses05M_scri...</td>\n",
              "      <td>/features/acoustic_features/Session5/Ses05M_sc...</td>\n",
              "      <td>/features/lexical_features/Session5/Ses05M_scr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1334</th>\n",
              "      <td>Ses05M_script03_2_M042</td>\n",
              "      <td>M05</td>\n",
              "      <td>/features/visual_features/Session5/Ses05M_scri...</td>\n",
              "      <td>/features/acoustic_features/Session5/Ses05M_sc...</td>\n",
              "      <td>/features/lexical_features/Session5/Ses05M_scr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1335</th>\n",
              "      <td>Ses05M_script03_2_M043</td>\n",
              "      <td>M05</td>\n",
              "      <td>/features/visual_features/Session5/Ses05M_scri...</td>\n",
              "      <td>/features/acoustic_features/Session5/Ses05M_sc...</td>\n",
              "      <td>/features/lexical_features/Session5/Ses05M_scr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              file_name_list  ... emotion_labels\n",
              "1331  Ses05M_script03_2_M029  ...              0\n",
              "1332  Ses05M_script03_2_M039  ...              0\n",
              "1333  Ses05M_script03_2_M041  ...              0\n",
              "1334  Ses05M_script03_2_M042  ...              0\n",
              "1335  Ses05M_script03_2_M043  ...              0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0IG908bnb0F"
      },
      "source": [
        "Class Imabalance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Xy7lBhsuOVj"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEZ67a_6uWwp",
        "outputId": "c8e34d5f-dfb2-4a24-a630-a94825648f08"
      },
      "source": [
        "Counter(data['emotion_labels'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 328, 1: 308, 2: 180, 3: 520})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBNNw8V1wB2S"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXjquEJInlmv"
      },
      "source": [
        "Pytorch Data loaders for all features reading the .npy files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ4ytPuAtcs5"
      },
      "source": [
        "import torch\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, list_IDs, labels,feature):\n",
        "        'Initialization'\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        self.feature_type=feature\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.list_IDs)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        ID = self.list_IDs[index]\n",
        "        number=ID[4]\n",
        "        val=ID.split(\"_\")\n",
        "        direc=\"\"\n",
        "        for i in range(len(val)-1):\n",
        "          direc+=val[i]\n",
        "          direc+=\"_\"\n",
        "        direc=direc[:-1]\n",
        "        # Load data and get label-features\n",
        "        if self.feature_type ==\"lexical_features\":\n",
        "          feature=np.load('features/'+self.feature_type+'/Session' + number+\"/\"+direc+\"/\"+ID + '.npy')\n",
        "        else:\n",
        "          feature=np.load('features/'+self.feature_type+'/Session' + number+\"/\"+direc+\"/\"+ID + '.npy')\n",
        "          feature=feature.mean(axis=0)\n",
        "\n",
        "        X =torch.tensor(feature)\n",
        "        y = self.labels[ID]\n",
        "\n",
        "        return X, y"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5KdgfFsnpW0"
      },
      "source": [
        "Creating separate training and validation sets for 10 fold cross validation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-eeMyFYtIuQ"
      },
      "source": [
        "def dataset_preparation(train,test):\n",
        "  params = {'batch_size': 32,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 1}\n",
        "  params_test = {'batch_size': 32,\n",
        "          'num_workers': 1}\n",
        "  labels={}\n",
        "  for i in train.iterrows():\n",
        "    \n",
        "    labels[i[1][0]]=i[1][5]\n",
        "\n",
        "  training_set_f1 = Dataset(train['file_name_list'].values, labels,\"visual_features\")\n",
        "  training_generator_f1 = torch.utils.data.DataLoader(training_set_f1, **params)\n",
        "  training_set_f2 = Dataset(train['file_name_list'].values, labels,\"acoustic_features\")\n",
        "  training_generator_f2 = torch.utils.data.DataLoader(training_set_f2, **params)\n",
        "  training_set_f3 = Dataset(train['file_name_list'].values, labels,\"lexical_features\")\n",
        "  training_generator_f3 = torch.utils.data.DataLoader(training_set_f3, **params)\n",
        "  labels={}\n",
        "  for i in test.iterrows():\n",
        "    labels[i[1][0]]=i[1][5]\n",
        "  testing_set_f1 = Dataset(test['file_name_list'].values, labels,\"visual_features\")\n",
        "  testing_generator_f1 = torch.utils.data.DataLoader(testing_set_f1, **params_test)\n",
        "  testing_set_f2 = Dataset(test['file_name_list'].values, labels,\"acoustic_features\")\n",
        "  testing_generator_f2 = torch.utils.data.DataLoader(testing_set_f2, **params_test)\n",
        "  testing_set_f3= Dataset(test['file_name_list'].values, labels,\"lexical_features\")\n",
        "  testing_generator_f3 = torch.utils.data.DataLoader(testing_set_f3, **params_test)\n",
        "  return training_generator_f1,training_generator_f2,training_generator_f3,testing_generator_f1,testing_generator_f2,testing_generator_f3"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jUFrYtTn1-f"
      },
      "source": [
        "Taking into account the label distribution in each training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuy4gHaZ4J2a"
      },
      "source": [
        "counter_speaker_wise={}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq-e4eD-Id3H"
      },
      "source": [
        "def extract(data,speaker):\n",
        "  val=\"speaker_\"+speaker\n",
        "  train=data[data[\"speakers\"]!=speaker]\n",
        "  test=data[data[\"speakers\"] ==speaker]\n",
        "  samples=Counter(train['emotion_labels'])\n",
        "  counter_speaker_wise[speaker]=list(samples.values())\n",
        "  training_generator_f1,training_generator_f2,training_generator_f3,testing_generator_f1,testing_generator_f2,testing_generator_f3=dataset_preparation(train,test)\n",
        "  return {\"train_visual\":training_generator_f1,\"train_acoustic\":training_generator_f2,\"train_lexical\":training_generator_f3,\"test_visual\":testing_generator_f1,\"test_acoustic\":testing_generator_f2,\"test_lexical\":testing_generator_f3}"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yARdaMZFH9jc"
      },
      "source": [
        "cv10_fold_data={}\n",
        "for i in data.speakers.unique():\n",
        "  cv10_fold_data[i]=extract(data,i)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLxSeevAn8Jh"
      },
      "source": [
        "A sample data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZsxNgMQvrvl",
        "outputId": "aabffc7b-cdae-461d-fd2e-324f4107f59c"
      },
      "source": [
        "for i in cv10_fold_data:\n",
        "  print(i)\n",
        "  print(cv10_fold_data[i]['test_lexical'])\n",
        "  for j,y in cv10_fold_data[i]['test_lexical']:\n",
        "    print(j,y)\n",
        "    break\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F01\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7f626d06abd0>\n",
            "tensor([[-1.1109, -0.0768, -0.4929,  ...,  0.8613,  0.8013, -0.6062],\n",
            "        [ 0.4657, -0.1908,  1.0678,  ..., -0.3106, -1.2877, -0.3745],\n",
            "        [ 0.5843,  1.0325, -1.3516,  ...,  0.8121, -0.5970,  1.4111],\n",
            "        ...,\n",
            "        [ 0.8674, -0.5370,  0.7117,  ...,  0.7471, -0.9581, -0.7356],\n",
            "        [ 1.4550,  1.1453,  0.1690,  ...,  0.2822,  0.4188,  1.3527],\n",
            "        [ 1.2102, -0.2770,  1.1886,  ..., -0.9448,  0.2593,  0.9709]]) tensor([3, 1, 3, 1, 1, 1, 3, 1, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 3, 1, 1, 3, 3, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIk2gDho-S9B",
        "outputId": "bf7a35b7-6be8-4e90-ed31-43b10c6b084c"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmBws9QfEbPl"
      },
      "source": [
        "Visual Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qI_9z3kKGt3"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from functools import partial\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vRfUMv7n-ny"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwn714oaIdwh"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self,l3=32,l2=256):\n",
        "      super(Net, self).__init__()\n",
        "    \n",
        "      self.fc1 = nn.Linear(2048, 1024)\n",
        "      self.fc2 = nn.Linear(1024, l2)\n",
        "      self.fc3 = nn.Linear(l2, l3)\n",
        "      self.fc4 = nn.Linear(l3, 4)\n",
        "\n",
        "    # x represents our data\n",
        "    def forward(self, x):\n",
        "      # Pass data through fc1\n",
        "\n",
        "   \n",
        "      x = self.fc1(x)\n",
        "  \n",
        "      x = self.fc2(x)\n",
        "      x = self.fc3(x)\n",
        "      x = self.fc4(x)\n",
        "\n",
        "      output = F.softmax(x, dim=1)\n",
        "      return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3MuoOpiN3fB"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E0T-xoUSUcq"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76TqxSeroAE5"
      },
      "source": [
        "Module for validation and finding F1 score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7cKtQ2Wjo9L"
      },
      "source": [
        "def find_f1(loader,model):\n",
        "  model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    y_true=[]\n",
        "    y_pred=[]\n",
        "    for i, samples in enumerate(loader, 0):\n",
        "        \n",
        "        inputs, labels = samples\n",
        "        inputs,labels = inputs.to(device),labels.cpu().detach().numpy()\n",
        "        outputs = model(inputs.float())\n",
        "        outputs=outputs.cpu().detach().numpy()\n",
        "       \n",
        "       \n",
        "        y_pred.extend(np.argmax(outputs,axis=1))\n",
        "        y_true.extend(labels)\n",
        "    return f1_score(y_true, y_pred, average='micro')  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4XGr4EyoESk"
      },
      "source": [
        "Training visual model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5kKeshX6sAJ"
      },
      "source": [
        "def train(l1,l2,learning_rate,speaker,loader,weights):\n",
        "  my_nn = Net(l1, l2)\n",
        "  my_nn = my_nn.to(device)\n",
        "  criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "  optimizer = optim.Adam(my_nn.parameters(), lr=learning_rate)\n",
        "  running_loss = []\n",
        "\n",
        "  for epoch in range(50):  \n",
        "      run_loss=0.0\n",
        "      \n",
        "    \n",
        "      for i, samples in enumerate(loader, 0):\n",
        "        \n",
        "        inputs, labels = samples\n",
        "        inputs,labels = inputs.to(device),labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        \n",
        "        outputs = my_nn(inputs.float())\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        run_loss+=loss.item()\n",
        "      #print(loss.item())\n",
        "     \n",
        "    \n",
        "  f1_scores = find_f1(cv10_fold_data[speaker]['test_visual'],my_nn)\n",
        "  return f1_scores,my_nn\n",
        "    \n",
        "\n",
        "  print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PXJujXkoG0i"
      },
      "source": [
        "Hyper-parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS7D7chR2wwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fe5fddb-f6e1-4d35-992c-b8bbc4eedec2"
      },
      "source": [
        "best_model={}\n",
        "best_f1={}\n",
        "hyper_parameter_results={}\n",
        "for l1 in [1024,512,256]:\n",
        "  for l2 in [256,64,32]:\n",
        "    if l1==512 and l2==256:\n",
        "      continue\n",
        "    if l1==256 and l2 ==256:\n",
        "      continue\n",
        "    for lr in [0.00001,0.0001]:\n",
        "      hyper_parameter_results[tuple([l1,l2,lr])]=[]\n",
        "      for k in cv10_fold_data:\n",
        "        \n",
        "        samples=counter_speaker_wise[k]\n",
        "        max_value=max(samples)\n",
        "        weights=[]\n",
        "        for i in samples:\n",
        "          weights.append(max_value/i)\n",
        "        weights=torch.tensor(weights).to(device)\n",
        "        loader=cv10_fold_data[k]['train_visual']\n",
        "        value,model = train(l1,l2,lr,k,loader,weights)\n",
        "        hyper_parameter_results[tuple([l1,l2,lr])].append([k,value])\n",
        "print(hyper_parameter_results)\n",
        "       \n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{(1024, 256, 1e-05): [['F01', 0.46308724832214765], ['M01', 0.45625], ['F02', 0.2288135593220339], ['M02', 0.36220472440944884], ['M03', 0.35625], ['F03', 0.35714285714285715], ['F04', 0.3333333333333333], ['M04', 0.3142857142857143], ['F05', 0.35185185185185186], ['M05', 0.38016528925619836]], (1024, 256, 0.0001): [['F01', 0.3959731543624161], ['M01', 0.4000000000000001], ['F02', 0.2627118644067797], ['M02', 0.3228346456692913], ['M03', 0.35], ['F03', 0.36507936507936506], ['F04', 0.25], ['M04', 0.3047619047619048], ['F05', 0.3395061728395062], ['M05', 0.34710743801652894]], (1024, 64, 1e-05): [['F01', 0.38926174496644295], ['M01', 0.43125], ['F02', 0.22033898305084745], ['M02', 0.4173228346456692], ['M03', 0.375], ['F03', 0.373015873015873], ['F04', 0.3055555555555556], ['M04', 0.3523809523809524], ['F05', 0.42592592592592593], ['M05', 0.34710743801652894]], (1024, 64, 0.0001): [['F01', 0.44966442953020136], ['M01', 0.4375], ['F02', 0.2457627118644068], ['M02', 0.39370078740157477], ['M03', 0.34375], ['F03', 0.3253968253968254], ['F04', 0.24074074074074073], ['M04', 0.3238095238095238], ['F05', 0.37654320987654316], ['M05', 0.35537190082644626]], (1024, 32, 1e-05): [['F01', 0.40939597315436244], ['M01', 0.40625], ['F02', 0.2457627118644068], ['M02', 0.39370078740157477], ['M03', 0.3375], ['F03', 0.373015873015873], ['F04', 0.2777777777777778], ['M04', 0.3523809523809524], ['F05', 0.3888888888888889], ['M05', 0.35537190082644626]], (1024, 32, 0.0001): [['F01', 0.40268456375838924], ['M01', 0.4375], ['F02', 0.288135593220339], ['M02', 0.29133858267716534], ['M03', 0.2875], ['F03', 0.38095238095238093], ['F04', 0.24074074074074073], ['M04', 0.3142857142857143], ['F05', 0.35185185185185186], ['M05', 0.4132231404958678]], (512, 64, 1e-05): [['F01', 0.38926174496644295], ['M01', 0.45], ['F02', 0.2711864406779661], ['M02', 0.37795275590551175], ['M03', 0.375], ['F03', 0.373015873015873], ['F04', 0.3333333333333333], ['M04', 0.3333333333333333], ['F05', 0.3888888888888889], ['M05', 0.36363636363636365]], (512, 64, 0.0001): [['F01', 0.3825503355704698], ['M01', 0.4000000000000001], ['F02', 0.2457627118644068], ['M02', 0.3228346456692913], ['M03', 0.35625], ['F03', 0.3253968253968254], ['F04', 0.25925925925925924], ['M04', 0.3142857142857143], ['F05', 0.35802469135802467], ['M05', 0.34710743801652894]], (512, 32, 1e-05): [['F01', 0.3959731543624161], ['M01', 0.47500000000000003], ['F02', 0.22033898305084745], ['M02', 0.39370078740157477], ['M03', 0.35], ['F03', 0.3968253968253968], ['F04', 0.2777777777777778], ['M04', 0.3333333333333333], ['F05', 0.4012345679012346], ['M05', 0.3140495867768595]], (512, 32, 0.0001): [['F01', 0.42953020134228187], ['M01', 0.44375], ['F02', 0.23728813559322035], ['M02', 0.33858267716535434], ['M03', 0.3125], ['F03', 0.38095238095238093], ['F04', 0.2777777777777778], ['M04', 0.3142857142857143], ['F05', 0.36419753086419754], ['M05', 0.38016528925619836]], (256, 64, 1e-05): [['F01', 0.3825503355704698], ['M01', 0.45], ['F02', 0.2457627118644068], ['M02', 0.4015748031496063], ['M03', 0.35625], ['F03', 0.38095238095238093], ['F04', 0.28703703703703703], ['M04', 0.34285714285714286], ['F05', 0.38271604938271603], ['M05', 0.36363636363636365]], (256, 64, 0.0001): [['F01', 0.436241610738255], ['M01', 0.4625], ['F02', 0.2627118644067797], ['M02', 0.36220472440944884], ['M03', 0.30625], ['F03', 0.373015873015873], ['F04', 0.3055555555555556], ['M04', 0.3238095238095238], ['F05', 0.3333333333333333], ['M05', 0.38016528925619836]], (256, 32, 1e-05): [['F01', 0.3959731543624161], ['M01', 0.45], ['F02', 0.23728813559322035], ['M02', 0.4251968503937008], ['M03', 0.34375], ['F03', 0.3888888888888889], ['F04', 0.25925925925925924], ['M04', 0.3238095238095238], ['F05', 0.3888888888888889], ['M05', 0.3305785123966942]], (256, 32, 0.0001): [['F01', 0.46308724832214765], ['M01', 0.4625], ['F02', 0.3389830508474576], ['M02', 0.30708661417322836], ['M03', 0.35], ['F03', 0.373015873015873], ['F04', 0.26851851851851855], ['M04', 0.3333333333333333], ['F05', 0.35185185185185186], ['M05', 0.33884297520661155]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fan5qfvhoJRH"
      },
      "source": [
        "Finding best results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8JP-25S1bN3",
        "outputId": "ef9b135d-e7a7-46ae-d944-0de442e1180f"
      },
      "source": [
        "final_result={}\n",
        "best_f1=-float(\"inf\")\n",
        "best_parameter=None\n",
        "for key in hyper_parameter_results:\n",
        "  f1=0\n",
        "  for speaker in hyper_parameter_results[key]:\n",
        "    f1+=speaker[1]\n",
        "  final_result[key]=f1/10\n",
        "  if f1>best_f1:\n",
        "    best_f1=f1\n",
        "    best_parameter=key\n",
        "\n",
        "print(final_result)\n",
        "print(best_f1/10)\n",
        "print(best_parameter)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{(1024, 256, 1e-05): 0.36033845779235857, (1024, 256, 0.0001): 0.33379745451357923, (1024, 64, 1e-05): 0.36371593075577957, (1024, 64, 0.0001): 0.34922401294462624, (1024, 32, 1e-05): 0.35400448653102823, (1024, 32, 0.0001): 0.34082125679824493, (512, 64, 1e-05): 0.3655608733757713, (512, 64, 0.0001): 0.33114716214205203, (512, 32, 1e-05): 0.355823358742944, (512, 32, 0.0001): 0.3479029707237125, (256, 64, 1e-05): 0.35933368244501235, (256, 64, 0.0001): 0.35457877745249683, (256, 32, 1e-05): 0.35436332135925924, (256, 32, 0.0001): 0.3587219465269022}\n",
            "0.3655608733757713\n",
            "(512, 64, 1e-05)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAIWZmoYoLkz"
      },
      "source": [
        "Saving models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz0BN78_GKY7"
      },
      "source": [
        "f1=0\n",
        "for k in cv10_fold_data:\n",
        "  loader=cv10_fold_data[k]['train_visual']\n",
        "  value = train(512, 64, 1e-05,k,loader)\n",
        "  f1+=value[0]\n",
        "  name=\"visual_\"+str(k)+\".pth\"\n",
        "  torch.save(value[1].state_dict(), name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxPB9uTMoOhq"
      },
      "source": [
        "Printing Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC1LdwiTY84A"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5XD2QdKZGzo"
      },
      "source": [
        "final_confusion_matrix_visual=[[0]*4 for _ in range(4)]\n",
        "confusion_matrix_visual=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57B2Pz9pQYpm"
      },
      "source": [
        "model = Net(512,64)\n",
        "for k in cv10_fold_data:\n",
        "  name=\"visual_\"+str(k)+\".pth\"\n",
        "  model.load_state_dict(torch.load(name))\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    y_true=[]\n",
        "    y_pred=[]\n",
        "    loader=cv10_fold_data[k]['test_visual']\n",
        "    for i, samples in enumerate(loader, 0):\n",
        "      inputs, labels = samples\n",
        "      inputs,labels = inputs,labels.cpu().detach().numpy()\n",
        "      outputs = model(inputs.float())\n",
        "      outputs=outputs.cpu().detach().numpy()\n",
        "      y_pred.extend(np.argmax(outputs,axis=1))\n",
        "      y_true.extend(labels)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(cm)\n",
        "    confusion_matrix_visual.append(cm)\n",
        "    final_confusion_matrix_visual+=cm\n",
        "     \n",
        "print(final_confusion_matrix_visual)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8lrj1SgeGs2",
        "outputId": "3d12a686-1712-454e-de96-c19b356aa853"
      },
      "source": [
        "print(final_confusion_matrix_visual)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[318 188  30 279]\n",
            " [155 300  42 269]\n",
            " [131 109  57 166]\n",
            " [366 322  91 603]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3fYIEw94fTX"
      },
      "source": [
        "import pickle\n",
        "\n",
        "f = open(\"file.pkl\",\"wb\")\n",
        "pickle.dump(hyper_parameter_results,f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3vvtjaTV4YS"
      },
      "source": [
        "*Textual Features* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik3l5vWboUkM"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s94Pvv1kV5Pq"
      },
      "source": [
        "class EmoGRU(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_units, batch_sz, output_size):\n",
        "        super(EmoGRU, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.hidden_units = hidden_units\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        # layers\n",
        "        #self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.hidden_units)\n",
        "        self.fc = nn.Linear(self.hidden_units, self.output_size)\n",
        "    \n",
        "    def initialize_hidden_state(self,batch_sz):\n",
        "        return torch.zeros((1, batch_sz, self.hidden_units))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        #x = self.embedding(x)\n",
        "        x=x.view(1,-1,768)\n",
        "        self.hidden = self.initialize_hidden_state(x.shape[1]).to(device)\n",
        "        output, self.hidden = self.gru(x, self.hidden) # max_len X batch_size X hidden_units\n",
        "        out = output[-1, :, :] \n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "        out = F.softmax(out, dim=1)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZWrJqNaoVq4"
      },
      "source": [
        "Training textual model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9jsiDlSb2_O"
      },
      "source": [
        "def train_lexical(units,learning_rate,speaker,loader,weights):\n",
        "  embedding_dim=768\n",
        "  \n",
        "  BATCH_SIZE=32\n",
        "  target_size=4\n",
        "\n",
        "  model = EmoGRU( embedding_dim, units, BATCH_SIZE, target_size)\n",
        "  criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  model = model.to(device)\n",
        "\n",
        "  running_loss = []\n",
        "\n",
        "  for epoch in range(50):  \n",
        "      run_loss=0.0\n",
        "      \n",
        "    \n",
        "      for i, samples in enumerate(loader, 0):\n",
        "        \n",
        "        inputs, labels = samples\n",
        "        inputs,labels = inputs.to(device),labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        \n",
        "        outputs = model(inputs.float())\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        run_loss+=loss.item()\n",
        "        \n",
        "      \n",
        "     \n",
        "    \n",
        "  f1_scores = find_f1(cv10_fold_data[speaker]['test_lexical'],model)\n",
        "  return f1_scores,model\n",
        "    \n",
        "\n",
        "  print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjcQUzjVoZve"
      },
      "source": [
        "Hyper-parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-bi6vbuccaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4420028-9b08-4eb5-d33c-4c6151a7cd1e"
      },
      "source": [
        "hyper_parameter_results_textual={}\n",
        "for l2 in [300, 100,64,32]:\n",
        "  for lr in [0.000001,0.00001,0.0001,0.001]:\n",
        "    hyper_parameter_results_textual[tuple([l2,lr])]=[]\n",
        "    for k in cv10_fold_data:\n",
        "       samples=counter_speaker_wise[k]\n",
        "        max_value=max(samples)\n",
        "        weights=[]\n",
        "        for i in samples:\n",
        "          weights.append(max_value/i)\n",
        "       weights=torch.tensor(weights).to(device)\n",
        "       loader=cv10_fold_data[k]['train_lexical']\n",
        "       value = train_lexical(l2,lr,k,loader,weights)\n",
        "       hyper_parameter_results_textual[tuple([l2,lr])].append([k,value[0]])\n",
        "       \n",
        "print(hyper_parameter_results_textual) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{(100, 1e-06): [['F01', 0.20134228187919462], ['M01', 0.3375], ['F02', 0.2288135593220339], ['M02', 0.37795275590551175], ['M03', 0.3], ['F03', 0.24603174603174602], ['F04', 0.39814814814814814], ['M04', 0.3523809523809524], ['F05', 0.20987654320987653], ['M05', 0.24793388429752067]], (100, 1e-05): [['F01', 0.5637583892617449], ['M01', 0.51875], ['F02', 0.5677966101694916], ['M02', 0.5590551181102362], ['M03', 0.4875], ['F03', 0.47619047619047616], ['F04', 0.5740740740740741], ['M04', 0.5619047619047619], ['F05', 0.5493827160493827], ['M05', 0.49586776859504134]], (100, 0.0001): [['F01', 0.610738255033557], ['M01', 0.6375], ['F02', 0.6016949152542372], ['M02', 0.7007874015748031], ['M03', 0.5875], ['F03', 0.5396825396825397], ['F04', 0.6481481481481481], ['M04', 0.580952380952381], ['F05', 0.6296296296296297], ['M05', 0.6611570247933884]], (100, 0.001): [['F01', 0.5906040268456376], ['M01', 0.6375], ['F02', 0.6440677966101694], ['M02', 0.6299212598425197], ['M03', 0.59375], ['F03', 0.5634920634920635], ['F04', 0.6944444444444444], ['M04', 0.6], ['F05', 0.6358024691358025], ['M05', 0.628099173553719]], (64, 1e-06): [['F01', 0.3758389261744966], ['M01', 0.26875], ['F02', 0.2711864406779661], ['M02', 0.29133858267716534], ['M03', 0.34375], ['F03', 0.36507936507936506], ['F04', 0.3148148148148148], ['M04', 0.26666666666666666], ['F05', 0.21604938271604937], ['M05', 0.2644628099173554]], (64, 1e-05): [['F01', 0.4899328859060403], ['M01', 0.5625], ['F02', 0.4152542372881356], ['M02', 0.5275590551181102], ['M03', 0.45], ['F03', 0.5238095238095238], ['F04', 0.5], ['M04', 0.4380952380952381], ['F05', 0.5493827160493827], ['M05', 0.5454545454545454]], (64, 0.0001): [['F01', 0.5973154362416108], ['M01', 0.6375], ['F02', 0.6101694915254238], ['M02', 0.6535433070866141], ['M03', 0.58125], ['F03', 0.5079365079365079], ['F04', 0.6574074074074074], ['M04', 0.5904761904761905], ['F05', 0.6481481481481481], ['M05', 0.6446280991735537]], (64, 0.001): [['F01', 0.6040268456375839], ['M01', 0.575], ['F02', 0.6016949152542372], ['M02', 0.6771653543307087], ['M03', 0.61875], ['F03', 0.5634920634920635], ['F04', 0.7037037037037037], ['M04', 0.580952380952381], ['F05', 0.6234567901234568], ['M05', 0.6611570247933884]], (32, 1e-06): [['F01', 0.2953020134228188], ['M01', 0.21875], ['F02', 0.2457627118644068], ['M02', 0.19685039370078738], ['M03', 0.3], ['F03', 0.23015873015873015], ['F04', 0.16666666666666666], ['M04', 0.2857142857142857], ['F05', 0.20987654320987653], ['M05', 0.2231404958677686]], (32, 1e-05): [['F01', 0.28187919463087246], ['M01', 0.4125], ['F02', 0.2796610169491525], ['M02', 0.36220472440944884], ['M03', 0.30625], ['F03', 0.40476190476190477], ['F04', 0.37962962962962965], ['M04', 0.3333333333333333], ['F05', 0.43209876543209874], ['M05', 0.4049586776859504]], (32, 0.0001): [['F01', 0.5637583892617449], ['M01', 0.64375], ['F02', 0.6101694915254238], ['M02', 0.6456692913385826], ['M03', 0.49375], ['F03', 0.5476190476190477], ['F04', 0.6296296296296297], ['M04', 0.5142857142857142], ['F05', 0.654320987654321], ['M05', 0.5950413223140496]], (32, 0.001): [['F01', 0.6241610738255033], ['M01', 0.6125], ['F02', 0.5932203389830508], ['M02', 0.6614173228346457], ['M03', 0.575], ['F03', 0.5634920634920635], ['F04', 0.6851851851851852], ['M04', 0.6190476190476191], ['F05', 0.6481481481481481], ['M05', 0.6198347107438017]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujRFn1uYocJc"
      },
      "source": [
        "Finding best results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGn8DWU9rdvW",
        "outputId": "a0f66b52-df76-4d2c-f1fc-2da27fb0e6c5"
      },
      "source": [
        "final_result={}\n",
        "best_f1=-float(\"inf\")\n",
        "best_parameter=None\n",
        "for key in hyper_parameter_results_textual:\n",
        "  f1=0\n",
        "  for speaker in hyper_parameter_results_textual[key]:\n",
        "    f1+=speaker[1]\n",
        "  final_result[key]=f1/10\n",
        "  if f1>best_f1:\n",
        "    best_f1=f1\n",
        "    best_parameter=key\n",
        "\n",
        "print(final_result)\n",
        "print(best_f1/10)\n",
        "print(best_parameter)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{(300, 1e-06): 0.40926304826095194, (300, 1e-05): 0.5760339107327832, (300, 0.0001): 0.6350955534644396, (300, 0.001): 0.6290531356463495, (250, 1e-06): 0.3977467082664123, (256, 1e-05): 0.5717796532127439, (256, 0.0001): 0.6222149595884938, (256, 0.001): 0.6241612782358648, (100, 1e-06): 0.28999798711749847, (100, 1e-05): 0.5354279914355209, (100, 0.0001): 0.6197790295068684, (100, 0.001): 0.6217681233924355, (64, 1e-06): 0.29779369887238794, (64, 1e-05): 0.5001988201720977, (64, 0.0001): 0.6128374587995455, (64, 0.001): 0.6209399078287523, (32, 1e-06): 0.23722218406053405, (32, 1e-05): 0.3597277246832391, (32, 0.0001): 0.5897993873628514, (32, 0.001): 0.6202006462260016}\n",
            "0.6350955534644396\n",
            "(300, 0.0001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAvKDVJmO0zZ"
      },
      "source": [
        "import pickle\n",
        "\n",
        "f = open(\"file_textual.pkl\",\"wb\")\n",
        "pickle.dump(hyper_parameter_results_textual,f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NErG8t-aog1d"
      },
      "source": [
        "Saving the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nJ-dSRKP76d"
      },
      "source": [
        "f1=0\n",
        "for k in cv10_fold_data:\n",
        "  loader=cv10_fold_data[k]['train_lexical']\n",
        "  samples=counter_speaker_wise[k]\n",
        "  max_value=max(samples)\n",
        "  weights=[]\n",
        "  for i in samples:\n",
        "    weights.append(max_value/i)\n",
        "  weights=torch.tensor(weights).to(device)\n",
        "  value = train_lexical(300,0.0001,k,loader,weights)\n",
        "  f1+=value[0]\n",
        "  name=\"lexical_\"+str(k)+\".pth\"\n",
        "  torch.save(value[1].state_dict(), name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHX76TDAokn2"
      },
      "source": [
        "Creating Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fStJGtvtdJCB"
      },
      "source": [
        "final_confusion_matrix_lexical=[[0]*4 for _ in range(4)]\n",
        "confusion_matrix_lexical=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBWdrEbmdYi8"
      },
      "source": [
        "model = EmoGRU( 768, 300, 32, 4)\n",
        "for k in cv10_fold_data:\n",
        "  name=\"lexical_\"+str(k)+\".pth\"\n",
        "  model.load_state_dict(torch.load(name))\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    y_true=[]\n",
        "    y_pred=[]\n",
        "    loader=cv10_fold_data[k]['test_lexical']\n",
        "    for i, samples in enumerate(loader, 0):\n",
        "      inputs, labels = samples\n",
        "      inputs,labels = inputs,labels.cpu().detach().numpy()\n",
        "      outputs = model(inputs.float())\n",
        "      outputs=outputs.cpu().detach().numpy()\n",
        "      y_pred.extend(np.argmax(outputs,axis=1))\n",
        "      y_true.extend(labels)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    confusion_matrix_lexical.append(cm)\n",
        "    final_confusion_matrix_lexical+=cm\n",
        "     \n",
        "print(final_confusion_matrix_lexical)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ygOYrxUe6Fx",
        "outputId": "60ef5b67-738b-41a3-afe2-329b9a384e49"
      },
      "source": [
        "print(final_confusion_matrix_lexical)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[229  17   9  73]\n",
            " [ 24 192  23  69]\n",
            " [ 21  27  64  68]\n",
            " [ 65  72  33 350]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LXWXmFAwF8R"
      },
      "source": [
        "Audio Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-eHdg1nooQK"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO746sRakLuM"
      },
      "source": [
        "class AudioGRU(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_units, batch_sz, output_size):\n",
        "        super(AudioGRU, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.hidden_units = hidden_units\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        # layers\n",
        "        #self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.hidden_units)\n",
        "        self.fc = nn.Linear(self.hidden_units, self.output_size)\n",
        "    \n",
        "    def initialize_hidden_state(self,batch_sz):\n",
        "        return torch.zeros((1, batch_sz, self.hidden_units))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        #x = self.embedding(x)\n",
        "        x=x.view(1,-1,128)\n",
        "        self.hidden = self.initialize_hidden_state(x.shape[1]).to(device)\n",
        "        output, self.hidden = self.gru(x, self.hidden) # max_len X batch_size X hidden_units\n",
        "        out = output[-1, :, :] \n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "        out = F.softmax(out, dim=1)\n",
        "        return out"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKNvlJXYop05"
      },
      "source": [
        "Training acoustic model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCY7SbLywQ1v"
      },
      "source": [
        "def train_audio(units,learning_rate,speaker,loader,weights):\n",
        "  embedding_dim=128\n",
        "  \n",
        "  BATCH_SIZE=32\n",
        "  target_size=4\n",
        "\n",
        "  model = AudioGRU(embedding_dim, units, BATCH_SIZE, target_size)\n",
        "  criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  model = model.to(device)\n",
        "\n",
        "  running_loss = []\n",
        "\n",
        "  for epoch in range(50):  \n",
        "      run_loss=0.0\n",
        "      \n",
        "    \n",
        "      for i, samples in enumerate(loader, 0):\n",
        "        \n",
        "        inputs, labels = samples\n",
        "        inputs,labels = inputs.to(device),labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        \n",
        "        outputs = model(inputs.float())\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        run_loss+=loss.item()\n",
        "        \n",
        "      \n",
        "     \n",
        "    \n",
        "  f1_scores = find_f1(cv10_fold_data[speaker]['test_acoustic'],model)\n",
        "  return f1_scores,model\n",
        "    \n",
        "\n",
        "  print('Finished Training')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwaSTLBAotn0"
      },
      "source": [
        "Hyper-parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym5CjM2hwc-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "724b5154-e1ca-49e5-dabc-39188f62be0a"
      },
      "source": [
        "hyper_parameter_results_audio={}\n",
        "for l2 in [100,64,32]:\n",
        "  for lr in [0.000001,0.00001,0.0001,0.001]:\n",
        "    hyper_parameter_results_audio[tuple([l2,lr])]=[]\n",
        "    for k in cv10_fold_data:\n",
        "       samples=counter_speaker_wise[k]\n",
        "        max_value=max(samples)\n",
        "        weights=[]\n",
        "        for i in samples:\n",
        "          weights.append(max_value/i)\n",
        "       loader=cv10_fold_data[k]['train_acoustic']\n",
        "       value = train_audio(l2,lr,k,loader,weights)\n",
        "       hyper_parameter_results_audio[tuple([l2,lr])].append([k,value[0]])\n",
        "print(hyper_parameter_results_audio)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{(100, 1e-06): [['F01', 0.20134228187919462], ['M01', 0.16875], ['F02', 0.23728813559322035], ['M02', 0.11811023622047244], ['M03', 0.25], ['F03', 0.23015873015873015], ['F04', 0.2037037037037037], ['M04', 0.22857142857142856], ['F05', 0.25925925925925924], ['M05', 0.2644628099173554]], (100, 1e-05): [['F01', 0.5167785234899329], ['M01', 0.525], ['F02', 0.4576271186440678], ['M02', 0.5118110236220472], ['M03', 0.5875], ['F03', 0.48412698412698413], ['F04', 0.4074074074074074], ['M04', 0.5238095238095238], ['F05', 0.5617283950617284], ['M05', 0.4628099173553719]], (100, 0.0001): [['F01', 0.5100671140939598], ['M01', 0.5375], ['F02', 0.5], ['M02', 0.5984251968503937], ['M03', 0.575], ['F03', 0.47619047619047616], ['F04', 0.5185185185185185], ['M04', 0.6476190476190476], ['F05', 0.5308641975308642], ['M05', 0.5289256198347108]], (100, 0.001): [['F01', 0.5302013422818792], ['M01', 0.55], ['F02', 0.5254237288135594], ['M02', 0.6220472440944882], ['M03', 0.55625], ['F03', 0.48412698412698413], ['F04', 0.5277777777777778], ['M04', 0.5714285714285714], ['F05', 0.5185185185185185], ['M05', 0.5371900826446281]], (64, 1e-06): [['F01', 0.3422818791946309], ['M01', 0.325], ['F02', 0.2966101694915254], ['M02', 0.39370078740157477], ['M03', 0.41875], ['F03', 0.25396825396825395], ['F04', 0.2962962962962963], ['M04', 0.2571428571428571], ['F05', 0.24691358024691357], ['M05', 0.3140495867768595]], (64, 1e-05): [['F01', 0.4563758389261745], ['M01', 0.43125], ['F02', 0.3983050847457627], ['M02', 0.5826771653543307], ['M03', 0.49375], ['F03', 0.4444444444444444], ['F04', 0.37962962962962965], ['M04', 0.47619047619047616], ['F05', 0.3271604938271605], ['M05', 0.4214876033057851]], (64, 0.0001): [['F01', 0.5100671140939598], ['M01', 0.5375], ['F02', 0.5], ['M02', 0.5669291338582677], ['M03', 0.5875], ['F03', 0.5], ['F04', 0.5277777777777778], ['M04', 0.6285714285714286], ['F05', 0.5493827160493827], ['M05', 0.512396694214876]], (64, 0.001): [['F01', 0.5637583892617449], ['M01', 0.5375], ['F02', 0.5], ['M02', 0.6220472440944882], ['M03', 0.525], ['F03', 0.5], ['F04', 0.5], ['M04', 0.5619047619047619], ['F05', 0.5370370370370371], ['M05', 0.5537190082644629]], (32, 1e-06): [['F01', 0.30201342281879195], ['M01', 0.23750000000000002], ['F02', 0.2542372881355932], ['M02', 0.18897637795275588], ['M03', 0.20625], ['F03', 0.31746031746031744], ['F04', 0.18518518518518517], ['M04', 0.2857142857142857], ['F05', 0.18518518518518517], ['M05', 0.2644628099173554]], (32, 1e-05): [['F01', 0.436241610738255], ['M01', 0.35625], ['F02', 0.288135593220339], ['M02', 0.4566929133858268], ['M03', 0.45625], ['F03', 0.4126984126984127], ['F04', 0.35185185185185186], ['M04', 0.4857142857142857], ['F05', 0.35185185185185186], ['M05', 0.30578512396694213]], (32, 0.0001): [['F01', 0.5369127516778524], ['M01', 0.525], ['F02', 0.4576271186440678], ['M02', 0.49606299212598426], ['M03', 0.58125], ['F03', 0.5], ['F04', 0.49074074074074076], ['M04', 0.5714285714285714], ['F05', 0.5370370370370371], ['M05', 0.49586776859504134]], (32, 0.001): [['F01', 0.5570469798657718], ['M01', 0.51875], ['F02', 0.4915254237288136], ['M02', 0.6456692913385826], ['M03', 0.525], ['F03', 0.4603174603174603], ['F04', 0.49074074074074076], ['M04', 0.5714285714285714], ['F05', 0.5370370370370371], ['M05', 0.5785123966942148]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrYSlQ-Sov_N"
      },
      "source": [
        "Finding best results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXv-eUepeQEH",
        "outputId": "5def5c6d-3837-40bd-b4d7-5041ce021e4b"
      },
      "source": [
        "final_result={}\n",
        "best_f1=-float(\"inf\")\n",
        "best_parameter=None\n",
        "for key in hyper_parameter_results_audio:\n",
        "  f1=0\n",
        "  for speaker in hyper_parameter_results_audio[key]:\n",
        "    f1+=speaker[1]\n",
        "  final_result[key]=f1/10\n",
        "  if f1>best_f1:\n",
        "    best_f1=f1\n",
        "    best_parameter=key\n",
        "\n",
        "print(final_result)\n",
        "print(best_f1/10)\n",
        "print(best_parameter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{(100, 1e-05): 0.4796765013736997, (100, 0.0001): 0.5049969358760285, (64, 1e-05): 0.4308027161942034, (64, 0.0001): 0.5003412835412266, (32, 1e-05): 0.3146908874911226, (32, 0.0001): 0.4848095382167544}\n",
            "0.5049969358760285\n",
            "(100, 0.0001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4exTpU2jozvV"
      },
      "source": [
        "Saving models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnsApzwcTYfT"
      },
      "source": [
        "f1=0\n",
        "for k in cv10_fold_data:\n",
        "  loader=cv10_fold_data[k]['train_acoustic']\n",
        "  samples=counter_speaker_wise[k]\n",
        "  max_value=max(samples)\n",
        "  weights=[]\n",
        "  for i in samples:\n",
        "    weights.append(max_value/i)\n",
        "  weights=torch.tensor(weights).to(device)\n",
        "  value = train_audio(100,0.0001,k,loader,weights)\n",
        "  f1+=value[0]\n",
        "  name=\"acoustic_new_\"+str(k)+\".pth\"\n",
        "  torch.save(value[1].state_dict(), name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWSEI0XFo15p"
      },
      "source": [
        "Creating confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnWeWJvte9LS"
      },
      "source": [
        "final_confusion_matrix_acoustic=[[0]*4 for _ in range(4)]\n",
        "confusion_matrix_acoustic=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phhulNCJfWEX"
      },
      "source": [
        "model = AudioGRU( 128, 100, 32, 4)\n",
        "for k in cv10_fold_data:\n",
        "  name=\"acoustic_new_\"+str(k)+\".pth\"\n",
        "  model.load_state_dict(torch.load(name))\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    y_true=[]\n",
        "    y_pred=[]\n",
        "    loader=cv10_fold_data[k]['test_acoustic']\n",
        "    for i, samples in enumerate(loader, 0):\n",
        "      inputs, labels = samples\n",
        "      inputs,labels = inputs,labels.cpu().detach().numpy()\n",
        "      outputs = model(inputs.float())\n",
        "      outputs=outputs.cpu().detach().numpy()\n",
        "      y_pred.extend(np.argmax(outputs,axis=1))\n",
        "      y_true.extend(labels)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    confusion_matrix_acoustic.append(cm)\n",
        "    final_confusion_matrix_acoustic+=cm\n",
        "     \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAx1PwWGhkJb",
        "outputId": "e5b0af78-1028-473c-b961-96853a49d97b"
      },
      "source": [
        "print(final_confusion_matrix_acoustic)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[214  20   7  87]\n",
            " [ 15 194   5  99]\n",
            " [ 40  39  58  47]\n",
            " [ 79 108   8 325]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CRzrzdcSeP-"
      },
      "source": [
        "Early Fusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trd0ZxL4o5iR"
      },
      "source": [
        "Creating new data loader to hold features in a concatenated form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1D5Sb45lwf31"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "class Dataset_Concat(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, list_IDs, labels):\n",
        "        'Initialization'\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        \n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.list_IDs)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        ID = self.list_IDs[index]\n",
        "        number=ID[4]\n",
        "        val=ID.split(\"_\")\n",
        "        direc=\"\"\n",
        "        for i in range(len(val)-1):\n",
        "          direc+=val[i]\n",
        "          direc+=\"_\"\n",
        "        direc=direc[:-1]\n",
        "       \n",
        "        feature_lexical=np.load('features/lexical_features/Session' + number+\"/\"+direc+\"/\"+ID + '.npy')\n",
        "        feature_acoustic=np.load('features/acoustic_features/Session' + number+\"/\"+direc+\"/\"+ID + '.npy')\n",
        "        feature_acoustic=feature_acoustic.mean(axis=0)\n",
        "        feature_visual=np.load('features/visual_features/Session' + number+\"/\"+direc+\"/\"+ID + '.npy')\n",
        "        feature_visual=feature_visual.mean(axis=0)\n",
        "        feature=np.concatenate((feature_lexical, feature_acoustic), axis=0)\n",
        "        feature=np.concatenate((feature, feature_visual), axis=0)\n",
        "        X =torch.tensor(feature)\n",
        "        y = self.labels[ID]\n",
        "\n",
        "        return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvR9C8RzUVWR"
      },
      "source": [
        "def dataset_preparation_concat(train,test):\n",
        "  params = {'batch_size': 128,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 1}\n",
        "\n",
        "  labels={}\n",
        "  for i in train.iterrows():\n",
        "    \n",
        "    labels[i[1][0]]=i[1][5]\n",
        "\n",
        "  training_set_f1 = Dataset_Concat(train['file_name_list'].values, labels)\n",
        "  training_generator_f1 = torch.utils.data.DataLoader(training_set_f1, **params)\n",
        "  \n",
        "  labels={}\n",
        "  for i in test.iterrows():\n",
        "    labels[i[1][0]]=i[1][5]\n",
        "  testing_set_f1 = Dataset_Concat(test['file_name_list'].values, labels)\n",
        "  testing_generator_f1 = torch.utils.data.DataLoader(testing_set_f1, **params)\n",
        "\n",
        "  return training_generator_f1, testing_generator_f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-YXxPXaUq1t"
      },
      "source": [
        "def extract_concat(data,speaker):\n",
        "  val=\"speaker_\"+speaker\n",
        "  train=data[data[\"speakers\"] !=speaker]\n",
        "  test=data[data[\"speakers\"] ==speaker]\n",
        "  training_generator, testing_generator = dataset_preparation_concat(train,test)\n",
        "  return [training_generator,testing_generator]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkrxNCRFUwRH"
      },
      "source": [
        "cv10_fold_concat_data={}\n",
        "for i in data.speakers.unique():\n",
        "  cv10_fold_concat_data[i]=extract_concat(data,i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZcvDLbnpE2H"
      },
      "source": [
        "Sample data loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkSvct4vWhME",
        "outputId": "af507911-4bb7-4f15-8fae-ff7367d5a957"
      },
      "source": [
        "for i in cv10_fold_concat_data:\n",
        "\n",
        "  for j,y in cv10_fold_concat_data[i][1]:\n",
        "    print(j.shape,y)\n",
        "    break\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 2944]) tensor([1, 3, 0, 3, 3, 1, 2, 0, 1, 1, 3, 2, 1, 0, 0, 0, 2, 0, 0, 1, 0, 1, 0, 2,\n",
            "        0, 3, 0, 2, 3, 2, 1, 2, 2, 3, 0, 0, 1, 2, 3, 0, 2, 0, 1, 0, 0, 0, 1, 0,\n",
            "        3, 3, 3, 3, 3, 1, 3, 2, 3, 3, 3, 2, 3, 0, 3, 3, 2, 0, 0, 0, 3, 0, 1, 3,\n",
            "        0, 2, 0, 3, 0, 0, 0, 0, 0, 3, 3, 0, 0, 2, 1, 0, 0, 3, 3, 3, 0, 0, 3, 2,\n",
            "        3, 3, 1, 1, 0, 0, 1, 0, 1, 0, 1, 2, 2, 0, 3, 3, 0, 3, 3, 0, 3, 3, 0, 3,\n",
            "        0, 2, 1, 1, 0, 2, 3, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27wPBwukpV-f"
      },
      "source": [
        "Model for Early Fusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYs26e0TWz-N"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc0 = nn.Linear(2944, 1024)\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "  \n",
        "        x = self.fc0(x)\n",
        "\n",
        "        x=x.view(-1,1,32,32)\n",
        "     \n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "    \n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "       \n",
        "        x = x.view(-1, 16 * 6 * 6)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0kdn-TKpZ-i"
      },
      "source": [
        "Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXxPySRSstm0"
      },
      "source": [
        "def train_multimodal(learning_rate,speaker,loader,weights):\n",
        "  my_nn = Net()\n",
        "  my_nn = my_nn.to(device)\n",
        "  criterion = nn.CrossEntropyLoss(weigth=weights)\n",
        "  optimizer = optim.Adam(my_nn.parameters(), lr=learning_rate)\n",
        "  running_loss = []\n",
        "\n",
        "  for epoch in range(50):  \n",
        "      run_loss=0.0\n",
        "      \n",
        "    \n",
        "      for i, samples in enumerate(loader, 0):\n",
        "        \n",
        "        inputs, labels = samples\n",
        "        inputs,labels = inputs.to(device),labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        \n",
        "        outputs = my_nn(inputs.float())\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        run_loss+=loss.item()\n",
        "        \n",
        "     \n",
        "    \n",
        "  f1_scores = find_f1(cv10_fold_concat_data[speaker][1],my_nn)\n",
        "  return f1_scores,my_nn\n",
        "    \n",
        "\n",
        "  print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO8Xub0dpbmq"
      },
      "source": [
        "Hyper-parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJI1mHCwtHPj",
        "outputId": "70d02b10-b73f-4e56-d540-f201b0801fd3"
      },
      "source": [
        "hyper_parameter_results_multimodal={}\n",
        "for lr in [1e-06, 1e-05,0.0001,0.001]:\n",
        "  hyper_parameter_results_multimodal[lr]=[]\n",
        "  for k in cv10_fold_concat_data:\n",
        "    samples=counter_speaker_wise[k]\n",
        "    max_value=max(samples)\n",
        "    weights=[]\n",
        "    for i in samples:\n",
        "      weights.append(max_value/i)\n",
        "    weights=torch.tensor(weights).to(device)\n",
        "    loader=cv10_fold_concat_data[k][0]\n",
        "    value = train_multimodal(lr,k,loader,weights)\n",
        "    hyper_parameter_results_multimodal[lr].append([k,value[0]])\n",
        "print(hyper_parameter_results_multimodal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1e-06: [['F01', 0.2953020134228188], ['M01', 0.1875], ['F02', 0.1440677966101695], ['M02', 0.5590551181102362], ['M03', 0.275], ['F03', 0.30952380952380953], ['F04', 0.18518518518518517], ['M04', 0.45714285714285713], ['F05', 0.4567901234567901], ['M05', 0.37190082644628103]], 1e-05: [['F01', 0.2953020134228188], ['M01', 0.6375], ['F02', 0.3474576271186441], ['M02', 0.6062992125984252], ['M03', 0.38125], ['F03', 0.48412698412698413], ['F04', 0.21296296296296297], ['M04', 0.45714285714285713], ['F05', 0.4567901234567901], ['M05', 0.4214876033057851]], 0.0001: [['F01', 0.6442953020134228], ['M01', 0.69375], ['F02', 0.635593220338983], ['M02', 0.7086614173228346], ['M03', 0.6], ['F03', 0.5714285714285714], ['F04', 0.6481481481481481], ['M04', 0.6190476190476191], ['F05', 0.5493827160493827], ['M05', 0.6363636363636364]], 0.001: [['F01', 0.6711409395973155], ['M01', 0.68125], ['F02', 0.6779661016949152], ['M02', 0.716535433070866], ['M03', 0.625], ['F03', 0.6190476190476191], ['F04', 0.6574074074074074], ['M04', 0.6952380952380952], ['F05', 0.5617283950617284], ['M05', 0.6363636363636364]]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z__A0hAdpd3y"
      },
      "source": [
        "Finding the best"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_VmZTbotOU7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a08a9a0-fe70-4e72-f2b2-3a0017304dd5"
      },
      "source": [
        "final_result={}\n",
        "best_f1=-float(\"inf\")\n",
        "best_parameter=None\n",
        "for key in hyper_parameter_results_multimodal:\n",
        "  f1=0\n",
        "  for speaker in hyper_parameter_results_multimodal[key]:\n",
        "    f1+=speaker[1]\n",
        "  final_result[key]=f1/10\n",
        "  if f1>best_f1:\n",
        "    best_f1=f1\n",
        "    best_parameter=key\n",
        "\n",
        "print(final_result)\n",
        "print(best_f1/10)\n",
        "print(best_parameter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1e-06: 0.3241467729898147, 1e-05: 0.43003193841352677, 0.0001: 0.6306670630712599, 0.001: 0.6541677627481584}\n",
            "0.6541677627481584\n",
            "0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7lOkPnCpix9"
      },
      "source": [
        "Saving Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehCv-XkHE8_c"
      },
      "source": [
        "f1=0\n",
        "for k in cv10_fold_concat_data:\n",
        "  loader=cv10_fold_concat_data[k][0]\n",
        "  samples=counter_speaker_wise[k]\n",
        "  max_value=max(samples)\n",
        "  weights=[]\n",
        "  for i in samples:\n",
        "    weights.append(max_value/i)\n",
        "  weights=torch.tensor(weights).to(device)\n",
        "  value = train_multimodal(0.001,k,loader,weights)\n",
        "  f1+=value[0]\n",
        "  name=\"multi_modal_concat\"+str(k)+\".pth\"\n",
        "  torch.save(value[1].state_dict(), name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a34n6P3mpkjU"
      },
      "source": [
        "Creating confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haKkZA3HhuID"
      },
      "source": [
        "final_confusion_matrix_early_fusion=[[0]*4 for _ in range(4)]\n",
        "confusion_matrix_early_fusion=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4MLzkqoh2ej"
      },
      "source": [
        "model = Net()\n",
        "for k in cv10_fold_concat_data:\n",
        "  name=\"multi_modal_concat\"+str(k)+\".pth\"\n",
        "  model.load_state_dict(torch.load(name))\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    y_true=[]\n",
        "    y_pred=[]\n",
        "    loader=cv10_fold_concat_data[k][1]\n",
        "    for i, samples in enumerate(loader, 0):\n",
        "      inputs, labels = samples\n",
        "      inputs,labels = inputs,labels.cpu().detach().numpy()\n",
        "      outputs = model(inputs.float())\n",
        "      outputs=outputs.cpu().detach().numpy()\n",
        "      y_pred.extend(np.argmax(outputs,axis=1))\n",
        "      y_true.extend(labels)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(cm)\n",
        "    confusion_matrix_early_fusion.append(cm)\n",
        "    final_confusion_matrix_early_fusion+=cm\n",
        "     \n",
        "print(final_confusion_matrix_early_fusion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-7sse72iJjk",
        "outputId": "239ca68a-a4f6-4c54-8eb4-c4c0afe4f667"
      },
      "source": [
        "print(final_confusion_matrix_early_fusion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[228  18  14  68]\n",
            " [ 20 213  21  54]\n",
            " [ 22  22  71  65]\n",
            " [ 60  67  42 351]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlS7In82iQgB"
      },
      "source": [
        "Late Fusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12TvTZmGBfKc"
      },
      "source": [
        "final_confusion_matrix_late_fusion=[[0]*4 for _ in range(4)]\n",
        "confusion_matrix_late_fusion=[]\n",
        "f1_scores_late_fusion=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwb2ajpDpovi"
      },
      "source": [
        "Loading all the best models and validating the output and creating confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91f25oAJp1t5"
      },
      "source": [
        "Majority Vote: by adding output probbalities and them using these to find the label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO1BCPKBiOgm"
      },
      "source": [
        "model_a=EmoGRU( 768, 300, 32, 4)\n",
        "model_b=AudioGRU( 128, 100, 32, 4)\n",
        "model_c=Net(512,64)\n",
        "for k in cv10_fold_data:\n",
        "  a,b,c =cv10_fold_data[k]['test_lexical'],cv10_fold_data[k]['test_acoustic'],cv10_fold_data[k]['test_visual']\n",
        "  name=\"lexical_\"+str(k)+\".pth\"\n",
        "  model_a.load_state_dict(torch.load(name))\n",
        "  model_a.eval()\n",
        "  name=\"acoustic_\"+str(k)+\".pth\"\n",
        "  model_b.load_state_dict(torch.load(name))\n",
        "  model_b.eval()\n",
        "  name=\"visual_\"+str(k)+\".pth\"\n",
        "  model_c.load_state_dict(torch.load(name))\n",
        "  model_c.eval()\n",
        "  with torch.no_grad():\n",
        "    y_true=[]\n",
        "    y_pred=[]\n",
        "    for i,j,k in zip( enumerate(a, 0), enumerate(b, 0), enumerate(c, 0)):\n",
        "        \n",
        "        inputs, labels = i[1][0],i[1][1]  \n",
        "        inputs,labels = inputs,labels.cpu().detach().numpy()\n",
        "        outputs_a = model_a(inputs.float())\n",
        "        outputs_a=outputs_a.cpu().detach().numpy()\n",
        "        \n",
        "        inputs, labels = j[1][0],j[1][1]\n",
        "        inputs,labels = inputs,labels.cpu().detach().numpy()\n",
        "        outputs_b = model_b(inputs.float())\n",
        "        outputs_b=outputs_b.cpu().detach().numpy()\n",
        "        \n",
        "        inputs, labels = k[1][0],k[1][1]\n",
        "        inputs,labels = inputs,labels.cpu().detach().numpy()\n",
        "        outputs_c = model_c(inputs.float())\n",
        "        outputs_c=outputs_c.cpu().detach().numpy()\n",
        "        \n",
        "        outputs=outputs_a+outputs_b+outputs_c\n",
        "       \n",
        "       \n",
        "        y_pred.extend(np.argmax(outputs,axis=1))\n",
        "        y_true.extend(labels)\n",
        "\n",
        "  f1 = f1_score(y_true, y_pred, average='micro')\n",
        "  f1_scores_late_fusion.append(f1)\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  confusion_matrix_late_fusion.append(cm)\n",
        "  final_confusion_matrix_late_fusion+=cm\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CagbA7CUCj8z",
        "outputId": "94d65c62-6d31-44b7-a37e-072c540cb676"
      },
      "source": [
        "print(final_confusion_matrix_late_fusion)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[240  10   0  78]\n",
            " [ 15 205   3  85]\n",
            " [ 30  24  29  97]\n",
            " [ 61  76   3 380]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkGDYUL57Q20",
        "outputId": "bfda544c-4cdd-420b-f934-354e12681d52"
      },
      "source": [
        "print(sum(f1_scores_late_fusion)/10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6395977983715335\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}